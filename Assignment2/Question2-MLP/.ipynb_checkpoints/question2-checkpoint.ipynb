{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0691afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score,precision_score,recall_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score \n",
    "np.set_printoptions(suppress=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9800669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    df=pd.read_csv(\"mnist_train.csv\")\n",
    "    x=df.iloc[:,1:]\n",
    "    y=df.iloc[:,0]\n",
    "    b = np.zeros((y.size, y.max()+1))\n",
    "    b[np.arange(y.size),y] = 1\n",
    "    y=b\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b516ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    \n",
    "    def __init__(self,nLayers,layerSize,activation,learningRate,weightInit,batchSize,epoch):\n",
    "        self.nLayers=nLayers\n",
    "        self.layerSize=layerSize\n",
    "        self.activation=activation\n",
    "        self.learningRate=learningRate\n",
    "        self.weightInit=weightInit\n",
    "        self.batchSize=batchSize\n",
    "        self.epoch=epoch\n",
    "        \n",
    "        bias=[]\n",
    "        for i in range(nLayers-1):\n",
    "            bias.append(np.zeros(layerSize[i+1]))\n",
    "        \n",
    "        weight=[]\n",
    "        for i in range(nLayers-1):\n",
    "            if(weightInit=='zero'):\n",
    "                weight.append(np.zero((layerSize[i],layerSize[i+1])))\n",
    "            elif(weightInit=='random'):\n",
    "                weight.append(np.random.rand(layerSize[i],layerSize[i+1])*0.01)\n",
    "            else:\n",
    "                weight.append(np.random.normal(size=(layerSize[i],layerSize[i+1]))*0.01)\n",
    "                \n",
    "        self.bias=bias\n",
    "        self.weight=weight\n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        if(x.all()>=0):\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def linear(self,x):\n",
    "        return x\n",
    "    \n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def softmax(self,x):\n",
    "#         print(\"softmaaaaaaaaaaaaaaax\",x.shape)\n",
    "        return np.exp(x)/(np.exp(x).sum(axis=1,keepdims=True))\n",
    "    \n",
    "    def tanhDerivative(self,x):\n",
    "        return 1-self.tanh(x)**2\n",
    "    \n",
    "    def ReLUDerivation(self,x):\n",
    "        if(x.all()>=0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sigmoidDerivative(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def linearDerivative(self,x):\n",
    "        return np.ones(x.shape)\n",
    "    \n",
    "    def softmaxDerivative(self,x):\n",
    "        return self.softmax(x)*(1-self.softmax(x))\n",
    "    \n",
    "    def leakyReLU(self,x):\n",
    "        return np.max(x,0.05*x)\n",
    "    \n",
    "    def leakyReLUDerivative(self,x):\n",
    "        if(x>=0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0.05\n",
    "    \n",
    "        \n",
    "    def activate(self,x):\n",
    "        \n",
    "        if(self.activation=='relu'):\n",
    "            return self.ReLU(x),self.ReLUDerivation(x)\n",
    "        elif(self.activation=='sigmoid'):\n",
    "            return self.sigmoid(x),self.sigmoidDerivative(x)\n",
    "        elif(self.activation=='tanh'):\n",
    "            return self.tanh(x),self.tanhDerivative(x)\n",
    "        elif(self.activation=='linear'):\n",
    "            return self.linear(x),self.linearDerivative(x)\n",
    "        elif(self.activation=='leakyrelu'):\n",
    "            return self.leakyReLU(x),self.leakyReLUDerivative(x)\n",
    "        else:\n",
    "            return self.softmax(x),self.softmaxDerivative(x)\n",
    "    \n",
    "    \n",
    "    def forwardPropagation(self,x):\n",
    "        \n",
    "        finalOut=x\n",
    "        allActivations=[]\n",
    "        allPreActivations=[]\n",
    "#         print(\"finalllllllllll\",finalOut)\n",
    "#         print(\"weightssssssssssssssss\",self.weight)\n",
    "        for i in range(len(self.weight)-1):\n",
    "#             print(\"ccccccccccccccccccccccccccccccc\",self.weight[i].shape,self.bias[i].shape)\n",
    "            layerActivation=np.dot(finalOut,self.weight[i])+self.bias[i]\n",
    "            finalOut=self.activate(layerActivation)[0]\n",
    "#             print(\"ssssssssssssssssssssssssssss\",finalOut.shape)\n",
    "            allActivations.append(finalOut)\n",
    "            allPreActivations.append(layerActivation)\n",
    "#         print(\"sssssssssssssss\",finalOut)    \n",
    "        layerActivation=np.dot(finalOut,self.weight[-1])+self.bias[-1]\n",
    "        finalOut=self.softmax(layerActivation)\n",
    "        allActivations.append(finalOut)\n",
    "        allPreActivations.append(layerActivation)\n",
    "            \n",
    "        return (allActivations,allPreActivations)\n",
    "         \n",
    "    def backwardPropagation(self,y,yPred,allActivations,allPreActivations):\n",
    "        \n",
    "        delta=yPred-y\n",
    "        derivatives=[]\n",
    "        derivatives.append(delta)\n",
    "#         print(\"checkkkkkkkkkkk\",allPreActivations)\n",
    "        for i in range(len(self.weight)-2,-1,-1):\n",
    "            e=np.dot(delta,self.weight[i+1].T)\n",
    "#             print(\"cccccccccccccccccccc\",e.shape)\n",
    "#             print('ssssssssssssssssssssssssss',allPreActivations[i].shape)\n",
    "            delta=e*self.activate(allPreActivations[i])[1]\n",
    "            \n",
    "            derivatives.append(delta)\n",
    "        \n",
    "        return derivatives[::-1]\n",
    "    \n",
    "            \n",
    "    def fit(self,x,y):\n",
    "        \n",
    "        allTrainError=[]\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            for size in range(0,x.shape[0],self.batchSize):\n",
    "                xNew=[]\n",
    "                yNew=[]\n",
    "                for temp in range(self.batchSize):\n",
    "                    xNew.append(x[size+temp])\n",
    "                    yNew.append(y[size+temp])\n",
    "#                 print(\"xnewwwwwwwwwwwwwwwwwwww\",np.array(xNew))\n",
    "                xNew=np.array(xNew)\n",
    "                yNew=np.array(yNew)\n",
    "                allActivations,allPreActivations=self.forwardPropagation(xNew)\n",
    "                yPred=allActivations[-1]\n",
    "                \n",
    "                derivatives=self.backwardPropagation(yNew,yPred,allActivations,allPreActivations)\n",
    "#                 allActivations[-1]=xNew\n",
    "                self.updateWeights(xNew,allActivations,derivatives)\n",
    "                \n",
    "            trainError=self.crossEntropyLoss(yPred,yNew)\n",
    "            allTrainError.append(trainError)\n",
    "            \n",
    "            if(((i+1)%5)==0):\n",
    "                print(\"For the epoch \",i+1,\"error = \",trainError)\n",
    "                \n",
    "        \n",
    "        return np.array(allTrainError)\n",
    "    \n",
    "    def updateWeights(self,xNew,allActivations,derivatives):\n",
    "        for layer in range(len(self.weight)):\n",
    "            if(layer==0):\n",
    "                grad=xNew.T.dot(derivatives[layer])/len(xNew)\n",
    "                self.weight[layer]=self.weight[layer]-self.learningRate*grad\n",
    "                self.bias[layer]=self.bias[layer]-self.learningRate*np.sum(derivatives[layer],axis=0)/len(xNew)\n",
    "            else:\n",
    "                grad=allActivations[layer-1].T.dot(derivatives[layer])/len(xNew)\n",
    "                self.weight[layer]=self.weight[layer]-self.learningRate*grad\n",
    "                self.bias[layer]=self.bias[layer]-self.learningRate*np.sum(derivatives[layer],axis=0)/len(xNew)\n",
    "            \n",
    "                \n",
    "        \n",
    "    def crossEntropyLoss(self,yPred,y):\n",
    "        logy=-np.log(yPred[np.arange(y.shape[0]),y.argmax(axis=1)])\n",
    "        return np.sum(logy)/y.shape[0]\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01bed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x,y=loadData()\n",
    "print(x.shape,y.shape)\n",
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ec58952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-ebb4ed884be9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMyNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'random'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-803268d5b0ea>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For the epoch \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"error = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "classifier=MyNeuralNetwork(5,[784,256, 128, 64,10],'relu',0.1,'random',3000,100)\n",
    "errors=classifier.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ff7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
