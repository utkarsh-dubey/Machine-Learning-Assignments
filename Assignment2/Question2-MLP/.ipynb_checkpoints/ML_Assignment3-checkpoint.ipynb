{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/itissandeep98/ML-Assignments/blob/master/Assignment3/ML_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Tx4Gs0XAbV6"
   },
   "source": [
    "#Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dtBFGh_qAgIg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "# from torchvision import models\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFx3VdqPNh9d"
   },
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rBDQnce8Njuq"
   },
   "outputs": [],
   "source": [
    "class MyPreProcessor():\n",
    "  \"\"\"\n",
    "  My steps for pre-processing for the All datasets.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def pre_process(self, dataset):\n",
    "    \"\"\"\n",
    "    Reading the file and preprocessing the input and output.\n",
    "    Note that you will encode any string value and/or remove empty entries in this function only.\n",
    "    Further any pre processing steps have to be performed in this function too. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    dataset : integer with acceptable values 0, 1, or 2\n",
    "    0 ->  Dataset\n",
    "    1 ->  Dataset\n",
    "    2 ->  Dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : 2-dimensional numpy array of shape (n_samples, n_features)\n",
    "    y : 1-dimensional numpy array of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "#     if dataset == 0:\n",
    "      # df=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\",header=None)\n",
    "      \n",
    "    df=pd.read_csv(\"mnist_train.csv\")\n",
    "    X=df.iloc[:,1:]\n",
    "    y=df.iloc[:,0]\n",
    "    b = np.zeros((y.size, y.max()+1))\n",
    "    b[np.arange(y.size),y] = 1\n",
    "    y=b\n",
    "           \n",
    "    \n",
    "#     elif dataset == 1:\n",
    "#       df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeTrain.csv\",header=None)\n",
    "#       X=df.iloc[:,1:].to_numpy()\n",
    "#       y=df[0].to_numpy()\n",
    "\n",
    "#     elif dataset == 2:\n",
    "#       df=pd.read_csv(\"/content/drive/MyDrive/ML_Assignment3/largeValidation.csv\",header=None)\n",
    "#       X=df.iloc[:,1:].to_numpy()\n",
    "#       y=df[0].to_numpy()\n",
    "    \n",
    "#     elif dataset == 3:\n",
    "#       df= pickle.load(open(\"/content/drive/MyDrive/ML_Assignment3/train_CIFAR.pickle\",\"rb\"))\n",
    "#       X=df['X']\n",
    "#       y=df['Y']\n",
    "#     elif dataset == 4:\n",
    "#       df= pickle.load(open(\"/content/drive/MyDrive/ML_Assignment3/test_CIFAR.pickle\",\"rb\"))\n",
    "#       X=df['X']\n",
    "#       y=df['Y']\n",
    "    return X, y\n",
    "\n",
    "preprocessor = MyPreProcessor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFXETQq-Ago6"
   },
   "source": [
    "#My Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wPTMl6_XAFan"
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    \"\"\"\n",
    "    My implementation of a Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh']\n",
    "    weight_inits = ['zero', 'random', 'normal']\n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
    "        \"\"\"\n",
    "        Initializing a new MyNeuralNetwork object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers : int value specifying the number of layers\n",
    "\n",
    "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
    "\n",
    "        activation : string specifying the activation function to be used\n",
    "                     possible inputs: relu, sigmoid, linear, tanh\n",
    "\n",
    "        learning_rate : float value specifying the learning rate to be used\n",
    "\n",
    "        weight_init : string specifying the weight initialization function to be used\n",
    "                      possible inputs: zero, random, normal\n",
    "\n",
    "        batch_size : int value specifying the batch size to be used\n",
    "\n",
    "        num_epochs : int value specifying the number of epochs to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if activation not in self.acti_fns:\n",
    "            raise Exception('Incorrect Activation Function')\n",
    "\n",
    "        if weight_init not in self.weight_inits:\n",
    "            raise Exception('Incorrect Weight Initialization Function')\n",
    "        \n",
    "        # np.random.seed(10)\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_sizes=layer_sizes \n",
    "        self.activation=activation \n",
    "        self.learning_rate=learning_rate \n",
    "        self.weight_init=weight_init\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        weights={}\n",
    "        bias={}\n",
    "        for i in range(self.n_layers-1):\n",
    "          weights[i]=np.array(self.weight_func((self.layer_sizes[i],self.layer_sizes[i+1])))\n",
    "          bias[i]=np.zeros(self.layer_sizes[i+1])\n",
    "\n",
    "        self.weights=weights\n",
    "        self.bias=bias\n",
    "        \n",
    "\n",
    "    def activation_func(self,X):\n",
    "      \"\"\"\n",
    "      Calculating the activation for a particular layer\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      X : 1-dimentional numpy array \n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "      x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "      x_derv : 1-dimensional numpy array after calculating the specified derivat function over X\n",
    "      \"\"\"\n",
    "      if self.activation==\"relu\":\n",
    "        return self.relu(X),self.relu_grad(X)\n",
    "      elif self.activation==\"sigmoid\":\n",
    "        return self.sigmoid(X),self.sigmoid_grad(X)\n",
    "      elif self.activation==\"linear\":\n",
    "        return self.linear(X),self.linear_grad(X)\n",
    "      elif self.activation==\"tanh\":\n",
    "        return self.tanh(X),self.tanh_grad(X)\n",
    "      else:\n",
    "        return self.softmax(X),self.softmax_grad(X)\n",
    "\n",
    "    def relu(self, X):\n",
    "      \"\"\"\n",
    "      Calculating the ReLU activation for a particular layer\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      X : 1-dimentional numpy array \n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "      x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "      \"\"\"\n",
    "      return X * (X>=0)\n",
    "\n",
    "    def relu_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "\n",
    "        return 1*(X>=0)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        x_calc= 1/(1+np.exp(-X))\n",
    "\n",
    "        return x_calc\n",
    "\n",
    "    def sigmoid_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        sig=self.sigmoid(X)\n",
    "        x_calc=sig*(1-sig)\n",
    "        return x_calc\n",
    "\n",
    "    def linear(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        x_calc=X\n",
    "        return x_calc\n",
    "\n",
    "    def linear_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        x_calc=np.ones(X.shape)\n",
    "        return x_calc\n",
    "\n",
    "    def tanh(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        x_calc=np.tanh(X)\n",
    "        return x_calc\n",
    "\n",
    "    def tanh_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        tanh=self.tanh(X)\n",
    "        x_calc=1-tanh**2\n",
    "        return x_calc\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        expo = np.exp(X)\n",
    "        x_calc=expo/expo.sum(axis=1, keepdims = True)\n",
    "        return x_calc\n",
    "    \n",
    "\n",
    "    def weight_func(self,shape):\n",
    "      if self.weight_init==\"zero\":\n",
    "        return self.zero_init(shape)\n",
    "      elif self.weight_init==\"random\":\n",
    "        return self.random_init(shape)\n",
    "      else:\n",
    "        return self.normal_init(shape)\n",
    "\n",
    "    def zero_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Zero Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight= np.zeros(shape)\n",
    "        return weight \n",
    "\n",
    "    def random_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Random Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight= np.random.rand(shape[0],shape[1])*0.01\n",
    "        return weight \n",
    "\n",
    "    def normal_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        weight=np.random.normal(size=shape)\n",
    "        return weight\n",
    "    \n",
    "    def cross_entropy(self,y_hat,y):\n",
    "        samples=y.shape[0]\n",
    "        error=y_hat-y\n",
    "        return error\n",
    "\n",
    "    def cross_entropy_loss(self, A, y):\n",
    "        n = len(y)\n",
    "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)])\n",
    "        loss = np.sum(logp)/n\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def fit(self, X, y,X_test=None,y_test=None):\n",
    "        \"\"\"\n",
    "        Fitting (training) the linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : an instance of self\n",
    "        \"\"\"\n",
    "        train_error=[]\n",
    "        test_error=[]\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "          for batch in range(0,X.shape[0],self.batch_size):\n",
    "            X_sample=deepcopy(X[batch:batch+self.batch_size,:])\n",
    "            y_sample=deepcopy(y[batch:batch+self.batch_size,:])\n",
    "            input=deepcopy(X_sample)\n",
    "            output=deepcopy(y_sample)\n",
    "\n",
    "            activations,preactivations = self.feed_forward(input)\n",
    "\n",
    "            dervs = self.backward_prop(output,activations,preactivations)\n",
    "\n",
    "            # Gradient updation\n",
    "            activations[-1]=X_sample\n",
    "            for layer in range(self.n_layers-1):\n",
    "              grad=activations[layer-1].T.dot(dervs[layer])/len(X_sample)\n",
    "              self.weights[layer]=self.weights[layer]-self.learning_rate*grad\n",
    "              self.bias[layer]=self.bias[layer]-self.learning_rate*np.sum(dervs[layer],axis=0)/len(X_sample)\n",
    "\n",
    "          if((epoch+1)%5==0):\n",
    "            train_cost = self.cross_entropy_loss(activations[self.n_layers-2],y_sample)\n",
    "            print(\"epoch\",epoch,\"\\t\",train_cost)\n",
    "          if(X_test is not None):\n",
    "            y_test_pred=self.predict_proba(X_test)\n",
    "            test_cost=self.cross_entropy_loss(y_test_pred,y_test)\n",
    "            print('hehehehehehehehehe',activations[self.n_layers-2])\n",
    "            train_cost = self.cross_entropy_loss(activations[self.n_layers-2],y_sample)\n",
    "            train_error.append(train_cost)\n",
    "            test_error.append(test_cost)\n",
    "            self.activations=activations\n",
    "            self.preactivations=preactivations\n",
    "\n",
    "        self.train_error=np.array(train_error)\n",
    "        self.test_error=np.array(test_error)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def feed_forward(self,input):\n",
    "      \"\"\"\n",
    "      Fitting (training) the linear model.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      input : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "      \n",
    "      Returns\n",
    "      -------\n",
    "      activations : dictionary of value of each layer after activation\n",
    "      preactivations : dictionary of value of each layer before activation\n",
    "      \"\"\"\n",
    "      preactivations={}\n",
    "      activations={}\n",
    "      for layer in range(self.n_layers-2):\n",
    "        hidden_output=input.dot(self.weights[layer])+self.bias[layer]\n",
    "        hidden_output_A,_=self.activation_func(hidden_output)          \n",
    "        input=hidden_output_A \n",
    "        preactivations[layer]=hidden_output  \n",
    "        activations[layer]=hidden_output_A\n",
    "\n",
    "      hidden_output=input.dot(self.weights[self.n_layers-2])+self.bias[self.n_layers-2]      \n",
    "#       print(\"cccccccccccccccccccccccccccccccc\",hidden_output)\n",
    "      preactivations[self.n_layers-2]=hidden_output \n",
    "      activations[self.n_layers-2]=self.softmax(hidden_output)\n",
    "      return activations,preactivations\n",
    "\n",
    "    def backward_prop(self,y,activations,preactivations):\n",
    "      \"\"\"\n",
    "      Fitting (training) the linear model.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      activations : dictionary of value of each layer after activation\n",
    "\n",
    "      preactivations : dictionary of value of each layer before activation\n",
    "      \n",
    "      Returns\n",
    "      -------\n",
    "      dervs: gradients that will be used to update weights and biases\n",
    "      \"\"\"\n",
    "      dervs={}\n",
    "      y_pred=activations[self.n_layers-2]\n",
    "      delta=y_pred-y\n",
    "      dervs[self.n_layers-2]=delta\n",
    "      for layer in range(self.n_layers-3,-1,-1):\n",
    "        error=delta.dot(self.weights[layer+1].T)\n",
    "        _,derv=self.activation_func(preactivations[layer])\n",
    "        delta=error*derv\n",
    "\n",
    "        dervs[layer]=delta\n",
    "\n",
    "      return dervs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicting probabilities using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
    "            class wise prediction probabilities.\n",
    "        \"\"\"\n",
    "        y,_=self.feed_forward(X)\n",
    "\n",
    "        # return the numpy array y which contains the probability of predicted values\n",
    "        return y[self.n_layers-2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
    "        \"\"\"\n",
    "        y=self.predict_proba(X)\n",
    "\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return y.argmax(axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        acc : float value specifying the accuracy of the model on the provided testing set\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred=self.predict(X)\n",
    "        y=y.argmax(axis=1)\n",
    "        acc=metrics.accuracy_score(y,y_pred)\n",
    "        return acc\n",
    "    \n",
    "    def asses(self):\n",
    "      plt.plot(range(self.num_epochs),self.train_error,label=\"Training error\")\n",
    "      plt.plot(range(self.num_epochs),self.test_error,label=\"Testing error\")\n",
    "      plt.legend()\n",
    "      plt.xlabel(\"Epochs\")\n",
    "      plt.ylabel(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEVsh_lHOIiv"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J3GmRAYX5HzM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X, y = preprocessor.pre_process(0)\n",
    "print(X.shape,y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.10)\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e3RRakSxOs59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 \t 2.1244154384099536\n",
      "epoch 9 \t 2.039606044912418\n",
      "epoch 14 \t 1.933403109417511\n",
      "epoch 19 \t 1.8073611900388877\n",
      "epoch 24 \t 1.7312705356132747\n",
      "epoch 29 \t 1.6803927540568668\n",
      "epoch 34 \t 1.6399341906140963\n",
      "epoch 39 \t 1.6048729351652566\n",
      "epoch 44 \t 1.5716936745745607\n",
      "epoch 49 \t 1.5383241834038692\n",
      "epoch 54 \t 1.503612262708266\n",
      "epoch 59 \t 1.4370594309377658\n",
      "epoch 64 \t 1.2174749010397874\n",
      "epoch 69 \t 1.095799185379938\n",
      "epoch 74 \t 1.0047127647893164\n",
      "epoch 79 \t 0.8912171611207341\n",
      "epoch 84 \t 0.7290263629891023\n",
      "epoch 89 \t 0.5804680510587241\n",
      "epoch 94 \t 0.4503427005972483\n",
      "epoch 99 \t 0.3225159036462767\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier=MyNeuralNetwork(5,[784,256, 128, 64,10],'relu',0.1,'random',3000,100)\n",
    "classifier.fit(X_train,y_train,X_test,y_test)\n",
    "classifier.asses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "k0IUbYnDT3mw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act,pre=classifier.feed_forward(X_train)\n",
    "a=act[2]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "08vsDh0YS8yl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 54000 samples in 0.010s...\n",
      "[t-SNE] Computed neighbors for 54000 samples in 78.017s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 31000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 32000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 33000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 34000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 35000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 36000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 37000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 38000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 39000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 40000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 41000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 42000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 43000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 44000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 45000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 46000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 47000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 48000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 49000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 50000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 51000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 52000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 53000 / 54000\n",
      "[t-SNE] Computed conditional probabilities for sample 54000 / 54000\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] Computed conditional probabilities in 3.183s\n",
      "[t-SNE] Iteration 50: error = 117.0479660, gradient norm = 0.0000062 (50 iterations in 24.643s)\n",
      "[t-SNE] Iteration 100: error = 101.6213989, gradient norm = 0.0031107 (50 iterations in 27.057s)\n",
      "[t-SNE] Iteration 150: error = 92.4972763, gradient norm = 0.0012960 (50 iterations in 24.599s)\n",
      "[t-SNE] Iteration 200: error = 88.7122650, gradient norm = 0.0010054 (50 iterations in 22.343s)\n",
      "[t-SNE] Iteration 250: error = 86.3210220, gradient norm = 0.0015470 (50 iterations in 24.484s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 86.321022\n",
      "[t-SNE] Iteration 300: error = 4.4163547, gradient norm = 0.0009512 (50 iterations in 23.180s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-580cc9d6deef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtsne_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utkarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utkarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    842\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded,\n",
      "\u001b[1;32mc:\\users\\utkarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter_without_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[1;32m--> 898\u001b[1;33m                                                           **opt_args)\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[1;31m# Save the final number of iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utkarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compute_error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\utkarsh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    265\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                                       \u001b[0mcompute_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m                                       num_threads=num_threads)\n\u001b[0m\u001b[0;32m    268\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, verbose=2, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(a)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXfNBuRmXuEG"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "  x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "  hue=y_train.argmax(axis=1),\n",
    "  palette=sns.color_palette(\"hls\", 10),\n",
    "  legend=\"full\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IVb2YY_XgXm"
   },
   "outputs": [],
   "source": [
    "pickle.dump(classifier,open(\"linear_random.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lijo6vgo9_UD"
   },
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN2sKcox-Am-"
   },
   "outputs": [],
   "source": [
    "# clf = MLPClassifier(activation=\"identity\", hidden_layer_sizes=(256, 128, 64),learning_rate_init=0.1,batch_size=3000, max_iter=100)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=0.1, hidden_layer_sizes=(256,128,64), random_state=1, activation = 'logistic')\n",
    "clf.fit(X_train,y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJNUNGUk_1nu"
   },
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_bsd1yBQYn9"
   },
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWPNACnui3cx"
   },
   "outputs": [],
   "source": [
    "X_train,y_train= preprocessor.pre_process(1)\n",
    "X_val,y_val= preprocessor.pre_process(2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqAbf-7va8SN"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "    super(MLP,self).__init__()\n",
    "    self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
    "    self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "  def forward(self, x):\n",
    "    h_1 = F.relu(self.input_fc(x))\n",
    "    y_pred = self.output_fc(h_1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwrWvudIzsAI"
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "  def __init__(self,X,y):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self,i):\n",
    "    return self.X[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei1QAlBBu48e"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iterator,val_iterator, optimizer, criterion, device,epochs,flag=True):  \n",
    "\n",
    "    ce_loss=[]\n",
    "    val_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      epoch_loss_val=0\n",
    "      for train,valid in zip(train_iterator,val_iterator):\n",
    "          # training\n",
    "          x,y = train\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          y_pred = model(x.float())\n",
    "          loss = criterion(y_pred, y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          epoch_loss += loss.item()\n",
    "\n",
    "          #validation\n",
    "          x_val,y_val = valid\n",
    "          x_val = x_val.to(device)\n",
    "          y_val = y_val.to(device)          \n",
    "          y_val_pred= model(x_val.float())\n",
    "          loss= criterion(y_val_pred, y_val)\n",
    "          epoch_loss_val += loss.item()\n",
    "      \n",
    "      ce_loss.append(epoch_loss/ len(train_iterator))\n",
    "      val_loss.append(epoch_loss_val/ len(val_iterator))\n",
    "\n",
    "      if((epoch+1)%100==0):\n",
    "        print(\"epoch:\",epoch+1,\"\\t\",epoch_loss/ len(train_iterator),epoch_loss_val/ len(val_iterator) )\n",
    "    if(not flag):\n",
    "      return ce_loss,val_loss\n",
    "    return np.mean(ce_loss),np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7ALiJVz-iEr"
   },
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSmxd7YJEUla"
   },
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fAm1UOWylxS"
   },
   "outputs": [],
   "source": [
    "hidden_units=[5, 20, 50, 100 ,200]\n",
    "input_size=128\n",
    "output_size=10\n",
    "\n",
    "ce_loss=[]\n",
    "val_loss=[]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "train_data=MyDataset(X_train,y_train)\n",
    "val_data=MyDataset(X_val,y_val)\n",
    "\n",
    "for h_unit in hidden_units:\n",
    "  print(h_unit)\n",
    "  train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
    "  val_iterator = data.DataLoader(val_data, batch_size = 126)\n",
    "  model = nn.Sequential(nn.Linear(input_size, h_unit),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(h_unit, output_size),\n",
    "                        nn.Softmax(dim=1))\n",
    "  # model=MLP(input_size,output_size,h_unit)\n",
    "  optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "  t_loss,v_loss=train(model,\n",
    "                      train_iterator,\n",
    "                      val_iterator,\n",
    "                      optimizer,\n",
    "                      criterion,\n",
    "                      device,\n",
    "                      500)\n",
    "  ce_loss.append(t_loss)\n",
    "  val_loss.append(v_loss)\n",
    "\n",
    "plt.plot(hidden_units,ce_loss,label=\"Average Training Loss\")\n",
    "plt.plot(hidden_units,val_loss,label=\"Average Validation Loss\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Number of Hidden Units\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR6Y8sqODueR"
   },
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vyNq-zLd4d2"
   },
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEje1AvmB-vz"
   },
   "outputs": [],
   "source": [
    "learning_rates=[0.1, 0.01, 0.001]\n",
    "input_size=128\n",
    "hidden_size=4\n",
    "output_size=10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_data=MyDataset(X_train,y_train)\n",
    "val_data=MyDataset(X_val,y_val)\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
    "val_iterator = data.DataLoader(val_data, shuffle = True, batch_size = 126)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_size, output_size),\n",
    "                        nn.Softmax(dim=1))\n",
    "for lr in learning_rates:\n",
    "  print(lr)\n",
    "  optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "  t_loss,v_loss=train(model,\n",
    "                      train_iterator,\n",
    "                      val_iterator,\n",
    "                      optimizer,\n",
    "                      criterion,\n",
    "                      device,\n",
    "                      100,\n",
    "                      False)\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.title(\"Learning Rate: \"+str(lr))\n",
    "  plt.plot(range(100),t_loss,label=\"Average Training Loss\")\n",
    "  # plt.plot(range(100),v_loss,label=\"Average Validation Loss\")\n",
    "  plt.ylabel(\"Cross Entropy Loss\")\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iquovfLLd-tH"
   },
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHFQIYp0bl2e"
   },
   "outputs": [],
   "source": [
    "X_train,y_train=preprocessor.pre_process(3)\n",
    "X_test,y_test=preprocessor.pre_process(4)\n",
    "\n",
    "X_train.shape,X_test.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DfwZa-ygokP"
   },
   "source": [
    "## 1) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IdxVvhHgVie"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(X_train)\n",
    "class_names=[\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWu7wXuhe0yM"
   },
   "outputs": [],
   "source": [
    "for i in range(16)\n",
    "  plt.subplot(2,8,i+1)\n",
    "  data = X_train[i, :] \n",
    "  data = np.reshape(data, (32,32,3), order='F' ) \n",
    "  plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bs1addzkW1p"
   },
   "outputs": [],
   "source": [
    "def display_color_hists(images, labels, indices, class_names=class_names):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    n = 0\n",
    "    for i in indices:\n",
    "        plt.subplot(2,3,n+1)\n",
    "        plt.hist(images[i][:1024],color = \"red\")\n",
    "        plt.title(class_names[labels[i]])\n",
    "        n += 1\n",
    "        \n",
    "        plt.subplot(2,3,n+1)\n",
    "        plt.hist(images[i][1024:2048],color = \"green\")\n",
    "        plt.title(class_names[labels[i]])\n",
    "        n += 1\n",
    "        \n",
    "        plt.subplot(2,3,n+1)\n",
    "        plt.hist(images[i][2048:],color = \"skyblue\")\n",
    "        plt.title(class_names[labels[i]])\n",
    "        n += 1\n",
    "    plt.show()\n",
    "display_color_hists(X_train, y_train,[0,2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7MmxUjTnaeG"
   },
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Woiuil05dUag"
   },
   "source": [
    "## 2) AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MBJ2BxRzpvD"
   },
   "outputs": [],
   "source": [
    "class MyDataset1(data.Dataset): \n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.img_shape = data.shape\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        img_reshaped = np.transpose(np.reshape(self.data[index],(3, 32,32)))\n",
    "  \n",
    "        img = Image.fromarray(img_reshaped)\n",
    "        label = self.label[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            img = img_to_tensor(img)\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYnLOJD1syrd"
   },
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.eval()\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxvepnO7xnbT"
   },
   "outputs": [],
   "source": [
    "train_transform_aug = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),       \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad(16),\n",
    "    transforms.Normalize( mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])]\n",
    "  )\n",
    "train_data=MyDataset1(X_train,y_train,train_transform_aug)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_data,\n",
    "                          batch_size=X_train.shape[0], \n",
    "                          shuffle=True)\n",
    "\n",
    "test_data=MyDataset1(X_test,y_test,train_transform_aug)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_data,\n",
    "                          batch_size=X_test.shape[0], \n",
    "                          shuffle=True)\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bthVaZAjs399"
   },
   "outputs": [],
   "source": [
    "for x,y in train_loader:\n",
    "  output=alexnet(x)\n",
    "  print(output.size())\n",
    "\n",
    "for x,y in test_loader:\n",
    "  output_test=alexnet(x)\n",
    "  print(output_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6fMbuUa2sR-"
   },
   "outputs": [],
   "source": [
    "X_new=output.detach().numpy()\n",
    "X_test_new=output_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAuYgCgWxpgn"
   },
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKi1sy8bwuJj"
   },
   "outputs": [],
   "source": [
    "class MyDataset2(data.Dataset):\n",
    "\n",
    "  def __init__(self,X,y):\n",
    "    self.X=X\n",
    "    self.y=y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self,i):\n",
    "    return self.X[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PorM2YJcy-u7"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device,epochs):  \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      epoch_loss = 0\n",
    "      for x,y in iterator:\n",
    "         \n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          y_pred = model(x.float())\n",
    "\n",
    "          model.zero_grad()\n",
    "\n",
    "          loss = criterion(y_pred, y)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "          epoch_loss += loss.item()\n",
    "      print(epoch_loss/len(iterator))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KPZdOzczjPL"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          torch.nn.Linear(1000, 512),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(512, 256),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(256, 2),\n",
    "        ).to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "train_data=MyDataset2(X_new,y_train)\n",
    "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
    "\n",
    "train(model,train_iterator,optimizer,criterion,device,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_CLZNBH4SOI"
   },
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZLuSEbV0jWq"
   },
   "outputs": [],
   "source": [
    "test_data=MyDataset2(X_test_new,y_test)\n",
    "test_iterator = data.DataLoader(test_data, shuffle = True, batch_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igLHtwb5BQDj"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for x,y in test_iterator:\n",
    "  output=model(x)\n",
    "  print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1OifQuFB7PV"
   },
   "outputs": [],
   "source": [
    "ypred=output.detach().numpy()\n",
    "ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA3ckf0XFYvb"
   },
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P91ALnHVFoRi"
   },
   "outputs": [],
   "source": [
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIP1HJzJzNsy"
   },
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxnErtVJzPzw"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256), max_iter=200, solver='sgd')\n",
    "mlp.fit(X_new, y_train)\n",
    "mlp.score(X_test_new,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cz3BLP1xuO"
   },
   "outputs": [],
   "source": [
    "y_pred=mlp.predict(X_test_new)\n",
    "metrics.confusion_matrix(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBAvU-zeeSXn"
   },
   "outputs": [],
   "source": [
    "scores = mlp.predict_proba(X_test_new)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, scores[:,0], pos_label=2)\n",
    "scores[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3NA_798e4su"
   },
   "outputs": [],
   "source": [
    "metrics.plot_roc_curve(mlp, X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQNyAKTcfSXE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nplsbN_6tFl"
   },
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVTiR3hp6vyw"
   },
   "outputs": [],
   "source": [
    "model=pickle.load(open(\"/content/sigmoid_normal.pkl\",\"rb\"))\n",
    "model.asses()\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHmylgPj67s7"
   },
   "outputs": [],
   "source": [
    "/content/linear_random.pkl\n",
    "/content/relu_random.pkl\n",
    "/content/sigmoid_normal.pkl\n",
    "/content/sigmoid_random.pkl\n",
    "/content/tanh_normal.pkl\n",
    "/content/tanh_random.pkl"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNAunbd7lKN2DpfFEhAxA0f",
   "collapsed_sections": [
    "1Tx4Gs0XAbV6",
    "CFx3VdqPNh9d",
    "iquovfLLd-tH",
    "7DfwZa-ygokP"
   ],
   "include_colab_link": true,
   "mount_file_id": "1QJfFz7Aj86zDZsXOVQqodupeap6Mw6fM",
   "name": "ML_Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
